<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <title>WGSL Live Shader Editor</title>
    <style>
        body {
            margin: 0;
            padding: 10px;
            background: #1e1e1e;
            color: #d4d4d4;
            font-family: system-ui, -apple-system, sans-serif;
        }
        #container {
            display: flex;
            gap: 10px;
            height: 98vh;
        }
        #leftPanel {
            flex: 1;
            display: flex;
            flex-direction: column;
            gap: 10px;
            min-width: 400px;
        }
        #rightPanel {
            display: flex;
            flex-direction: column;
            gap: 10px;
        }
        #shaderCode {
            flex: 1;
            background: #1e1e1e;
            color: #d4d4d4;
            border: 1px solid #3c3c3c;
            border-radius: 4px;
            padding: 10px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 13px;
            line-height: 1.5;
            resize: none;
            tab-size: 4;
        }
        #shaderCode:focus {
            outline: 2px solid #007acc;
            border-color: #007acc;
        }
        #controls {
            display: flex;
            gap: 10px;
            align-items: center;
            flex-wrap: wrap;
        }
        button {
            background: #0e639c;
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            font-weight: 500;
        }
        button:hover {
            background: #1177bb;
        }
        button:disabled {
            background: #3c3c3c;
            cursor: not-allowed;
        }
        #errorDisplay {
            background: #1e1e1e;
            border: 1px solid #3c3c3c;
            border-radius: 4px;
            padding: 10px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 12px;
            min-height: 60px;
            max-height: 120px;
            overflow-y: auto;
        }
        .error {
            color: #f48771;
        }
        .success {
            color: #4ec9b0;
        }
        .info {
            color: #9cdcfe;
        }
        #canvas {
            border: 1px solid #3c3c3c;
            border-radius: 4px;
            image-rendering: pixelated;
        }
        .hint {
            color: #858585;
            font-size: 12px;
        }
    </style>
</head>
<body>
    <div id="container">
        <div id="leftPanel">
            <div id="controls">
                <button id="reloadBtn">Reload Shader (Ctrl+Enter)</button>
                <button id="audioBtn">Enable Audio</button>
                <label style="color: #858585;">
                    Volume: <input type="range" id="volumeSlider" min="0" max="100" value="50" style="width: 100px;">
                    <span id="volumeValue">50%</span>
                </label>
            </div>
            <textarea id="shaderCode" spellcheck="false"></textarea>
            <div>
                <div class="hint">Status:</div>
                <div id="errorDisplay" class="info">Ready to compile...</div>
            </div>
        </div>
        <div id="rightPanel">
            <canvas id="canvas"></canvas>
            <div class="hint">
                Audio: <span id="audioStatus">Waiting...</span><br>
                Frame: <span id="frameCounter">0</span><br>
                Time: <span id="timeCounter">0.00s</span>
            </div>
        </div>
    </div>

    <script>
    'use strict';

    // ============================================================================
    // Configuration & Constants
    // ============================================================================
    const CONFIG = {
        sampleRate: 48000,
        audioBlockDuration: 0.1,
        channels: 2,
        volume: 0.5,
        screenSize: 512,
        computeThreads: 64,
        computeBufferSize: 2 << 20,
    };

    const DERIVED = {
        samplesPerBlock: (CONFIG.audioBlockDuration * CONFIG.sampleRate) | 0,
        audioBufferSize: 0,
    };
    DERIVED.audioBufferSize = 4 * CONFIG.channels * DERIVED.samplesPerBlock;

    const UNIFORM_STRUCT = {
        time: 0,
        audioCurrentTime: 1,
        audioPlayTime: 2,
        audioFractTime: 3,
        audioFrame: 4,
        SIZE: 5,
    };

    // ============================================================================
    // Application State
    // ============================================================================
    const state = {
        gpuDevice: null,
        audioContext: null,
        gainNode: null,
        
        canvas: null,
        gpuContext: null,
        bindGroupLayout: null,
        pipeline: null,
        uniformBuffer: null,
        computeBuffer: null,
        audioBufferGPU: null,
        audioBuffersReadback: [null, null],
        
        lastFrameTime: 0,
        audioFrame: 0,
        nextAudioTime: 0,
        
        readbackIndex: 0,
        pendingAudio: false,
        isRunning: false,
    };

    // ============================================================================
    // Default WGSL Shader
    // ============================================================================
    const DEFAULT_WGSL = `const SAMPLE_RATE = ${CONFIG.sampleRate}f;
const SAMPLES_PER_BLOCK = ${DERIVED.samplesPerBlock};
const COMPUTE_THREADS = ${CONFIG.computeThreads};
const SCREEN_SIZE = ${CONFIG.screenSize};
const PI = 3.1415926535897932f;
const TAU = 6.283185307179586f;

struct Uniforms {
    time: f32,
    audioCurrentTime: f32,
    audioPlayTime: f32,
    audioFractTime: f32,
    audioFrame: i32,
}

@binding(0) @group(0) var<uniform> uniforms: Uniforms;
@binding(1) @group(0) var<storage, read_write> computeBuffer: array<f32>;
@binding(2) @group(0) var<storage, read_write> audioBuffer: array<f32>;
@binding(3) @group(0) var screenTexture: texture_storage_2d<bgra8unorm, write>;

@compute @workgroup_size(COMPUTE_THREADS, 1, 1)
fn main(@builtin(global_invocation_id) gid: vec3<u32>) {
    let threadId = gid.x + gid.y * u32(SCREEN_SIZE);
    
    // ========================================================================
    // Graphics: Write to screen texture
    // ========================================================================
    if (gid.x < u32(SCREEN_SIZE) && gid.y < u32(SCREEN_SIZE)) {
        let uv = vec2f(gid.xy) / f32(SCREEN_SIZE);
        let t = uniforms.time;
        
        // Animated pattern synced to audio
        let audioPhase = uniforms.audioFractTime * TAU * 440.0;
        let pattern = cos(uv * 10.0 + t);
        let pulse = sin(audioPhase) * 0.5 + 0.5;
        
        let color = vec4f(
            pattern.x * 0.5 + 0.5,
            pattern.y * 0.5 + 0.5,
            pulse,
            1.0
        );
        
        textureStore(screenTexture, gid.xy, color);
    }
    
    // ========================================================================
    // Audio: Generate samples
    // ========================================================================
    let sampleIndex = i32(threadId);
    if (sampleIndex < SAMPLES_PER_BLOCK) {
        // TIME PRECISION FIX: Use fractional seconds to avoid float drift
        let sampleTime = uniforms.audioFractTime + f32(sampleIndex) / SAMPLE_RATE;
        
        // 440Hz sine wave (A4 note)
        let frequency = 440.0;
        let phase = sampleTime * frequency * TAU;
        let sample = sin(phase) * 0.3;
        
        // Write to interleaved stereo buffer
        audioBuffer[sampleIndex] = sample;
        audioBuffer[SAMPLES_PER_BLOCK + sampleIndex] = sample;
    }
}`;

    // ============================================================================
    // Initialization
    // ============================================================================
    async function init() {
        setupUI();
        try {
            await initWebGPU();
            initWebAudio();
            await reloadShader();
            state.isRunning = true;
            requestAnimationFrame(render);
            logStatus('System initialized successfully!', 'success');
        } catch (err) {
            logStatus('Initialization failed: ' + err.message, 'error');
        }
    }

    function setupUI() {
        // Canvas
        state.canvas = document.getElementById('canvas');
        state.canvas.width = CONFIG.screenSize;
        state.canvas.height = CONFIG.screenSize;
        state.canvas.style.width = CONFIG.screenSize + 'px';
        state.canvas.style.height = CONFIG.screenSize + 'px';

        // Shader code editor
        const editor = document.getElementById('shaderCode');
        editor.value = DEFAULT_WGSL;
        
        // Tab key support
        editor.addEventListener('keydown', (e) => {
            if (e.key === 'Tab') {
                e.preventDefault();
                const start = editor.selectionStart;
                const end = editor.selectionEnd;
                editor.value = editor.value.substring(0, start) + '    ' + editor.value.substring(end);
                editor.selectionStart = editor.selectionEnd = start + 4;
            }
            // Ctrl+Enter to reload
            if (e.ctrlKey && e.key === 'Enter') {
                e.preventDefault();
                reloadShader();
            }
        });

        // Reload button
        document.getElementById('reloadBtn').addEventListener('click', reloadShader);

        // Audio button
        const audioBtn = document.getElementById('audioBtn');
        audioBtn.addEventListener('click', () => {
            state.audioContext?.resume();
            audioBtn.textContent = 'Audio Enabled âœ“';
            audioBtn.disabled = true;
        });

        // Volume control
        const volumeSlider = document.getElementById('volumeSlider');
        const volumeValue = document.getElementById('volumeValue');
        volumeSlider.addEventListener('input', (e) => {
            const vol = e.target.value / 100;
            CONFIG.volume = vol;
            if (state.gainNode) state.gainNode.gain.value = vol;
            volumeValue.textContent = e.target.value + '%';
        });

        // Handle visibility change to fix audio timing issues
        document.addEventListener('visibilitychange', () => {
            if (!document.hidden && state.audioContext) {
                // Page became visible - resync audio timing
                const ctx = state.audioContext;
                state.nextAudioTime = Math.ceil(ctx.currentTime / CONFIG.audioBlockDuration) 
                                     * CONFIG.audioBlockDuration;
                state.pendingAudio = false;
                logStatus('Audio resynced after page visibility change', 'info');
            }
        });
    }

    async function initWebGPU() {
        const adapter = await navigator.gpu?.requestAdapter();
        if (!adapter) throw new Error('WebGPU not supported');

        state.gpuDevice = await adapter.requestDevice({
            requiredFeatures: ['bgra8unorm-storage'],
            requiredLimits: {
                maxBufferSize: Math.pow(2, 30),
                maxStorageBufferBindingSize: Math.pow(2, 30),
            }
        });

        state.gpuContext = state.canvas.getContext('webgpu');
        state.gpuContext.configure({
            device: state.gpuDevice,
            format: navigator.gpu.getPreferredCanvasFormat(),
            usage: GPUTextureUsage.STORAGE_BINDING,
        });

        createGPUResources();
    }

    function createGPUResources() {
        const device = state.gpuDevice;

        // Buffers
        state.uniformBuffer = device.createBuffer({
            size: 256,
            usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
        });

        state.computeBuffer = device.createBuffer({
            size: CONFIG.computeBufferSize,
            usage: GPUBufferUsage.STORAGE,
        });

        state.audioBufferGPU = device.createBuffer({
            size: DERIVED.audioBufferSize,
            usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC,
        });

        for (let i = 0; i < 2; i++) {
            state.audioBuffersReadback[i] = device.createBuffer({
                size: DERIVED.audioBufferSize,
                usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,
            });
        }

        // Bind group layout
        state.bindGroupLayout = device.createBindGroupLayout({
            entries: [
                { binding: 0, visibility: GPUShaderStage.COMPUTE, buffer: { type: 'uniform' } },
                { binding: 1, visibility: GPUShaderStage.COMPUTE, buffer: { type: 'storage' } },
                { binding: 2, visibility: GPUShaderStage.COMPUTE, buffer: { type: 'storage' } },
                { binding: 3, visibility: GPUShaderStage.COMPUTE, storageTexture: { 
                    format: 'bgra8unorm', access: 'write-only', viewDimension: '2d' 
                }},
            ],
        });
    }

    function initWebAudio() {
        state.audioContext = new AudioContext();
        state.gainNode = state.audioContext.createGain();
        state.gainNode.gain.value = CONFIG.volume;
        state.gainNode.connect(state.audioContext.destination);
        
        state.nextAudioTime = Math.ceil(state.audioContext.currentTime / CONFIG.audioBlockDuration) 
                             * CONFIG.audioBlockDuration;
        
        playUnlockSound();
    }

    function playUnlockSound() {
        const ctx = state.audioContext;
        const buffer = ctx.createBuffer(2, ctx.sampleRate * 0.05, ctx.sampleRate);
        for (let ch = 0; ch < 2; ch++) {
            const data = buffer.getChannelData(ch);
            for (let i = 0; i < data.length; i++) {
                data[i] = (Math.random() * 2 - 1) * 0.05;
            }
        }
        const source = ctx.createBufferSource();
        source.buffer = buffer;
        source.connect(state.gainNode);
        source.start();
    }

    // ============================================================================
    // Live Shader Reload
    // ============================================================================
    async function reloadShader() {
        const code = document.getElementById('shaderCode').value;
        
        try {
            logStatus('Compiling shader...', 'info');
            
            const shaderModule = state.gpuDevice.createShaderModule({ code });
            
            // Check for compilation errors
            const compilationInfo = await shaderModule.getCompilationInfo();
            const errors = compilationInfo.messages.filter(m => m.type === 'error');
            
            if (errors.length > 0) {
                const errorMsg = errors.map(e => 
                    `Line ${e.lineNum}: ${e.message}`
                ).join('\n');
                throw new Error('Shader compilation failed:\n' + errorMsg);
            }
            
            // Create new pipeline
            const newPipeline = state.gpuDevice.createComputePipeline({
                layout: state.gpuDevice.createPipelineLayout({
                    bindGroupLayouts: [state.bindGroupLayout],
                }),
                compute: { module: shaderModule, entryPoint: 'main' },
            });
            
            // Swap pipeline (audio continues uninterrupted!)
            state.pipeline = newPipeline;
            
            logStatus('âœ“ Shader compiled successfully!', 'success');
        } catch (err) {
            logStatus('âœ— ' + err.message, 'error');
            throw err;
        }
    }

    function logStatus(message, type = 'info') {
        const display = document.getElementById('errorDisplay');
        display.textContent = message;
        display.className = type;
    }

    // ============================================================================
    // Render Loop
    // ============================================================================
    function render(time) {
        if (!state.isRunning) return;

        const device = state.gpuDevice;
        const ctx = state.audioContext;
        
        // Update UI
        document.getElementById('frameCounter').textContent = state.audioFrame;
        document.getElementById('timeCounter').textContent = (time * 0.001).toFixed(2) + 's';
        document.getElementById('audioStatus').textContent = 
            ctx.state === 'running' ? 'ðŸ”Š Playing' : 'ðŸ”‡ Suspended';

        // Update uniforms
        const uniformData = new ArrayBuffer(256);
        const uniformF32 = new Float32Array(uniformData);
        const uniformI32 = new Int32Array(uniformData);
        
        uniformF32[UNIFORM_STRUCT.time] = time * 0.001;
        uniformF32[UNIFORM_STRUCT.audioCurrentTime] = ctx.currentTime;
        uniformF32[UNIFORM_STRUCT.audioPlayTime] = state.nextAudioTime;
        uniformF32[UNIFORM_STRUCT.audioFractTime] = state.nextAudioTime % 1;
        uniformI32[UNIFORM_STRUCT.audioFrame] = state.audioFrame;
        
        device.queue.writeBuffer(state.uniformBuffer, 0, uniformData);

        // Create bind group
        const textureView = state.gpuContext.getCurrentTexture().createView();
        const bindGroup = device.createBindGroup({
            layout: state.bindGroupLayout,
            entries: [
                { binding: 0, resource: { buffer: state.uniformBuffer } },
                { binding: 1, resource: { buffer: state.computeBuffer } },
                { binding: 2, resource: { buffer: state.audioBufferGPU } },
                { binding: 3, resource: textureView },
            ],
        });

        // Dispatch compute shader
        const encoder = device.createCommandEncoder();
        const pass = encoder.beginComputePass();
        pass.setPipeline(state.pipeline);
        pass.setBindGroup(0, bindGroup);
        pass.dispatchWorkgroups(
            CONFIG.screenSize / CONFIG.computeThreads,
            CONFIG.screenSize,
            1
        );
        pass.end();

        // Audio transfer timing
        if (ctx.currentTime >= state.nextAudioTime - CONFIG.audioBlockDuration && !state.pendingAudio) {
            state.pendingAudio = true;
            
            const readbackBuffer = state.audioBuffersReadback[state.readbackIndex];
            encoder.copyBufferToBuffer(
                state.audioBufferGPU, 0,
                readbackBuffer, 0,
                DERIVED.audioBufferSize
            );
            
            device.queue.submit([encoder.finish()]);
            
            readbackBuffer.mapAsync(GPUMapMode.READ).then(() => {
                playAudioBlock(readbackBuffer);
                state.readbackIndex = 1 - state.readbackIndex;
                state.pendingAudio = false;
            }).catch(err => {
                console.error('Audio readback failed:', err);
                state.pendingAudio = false;
            });
        } else {
            device.queue.submit([encoder.finish()]);
        }

        requestAnimationFrame(render);
    }

    function playAudioBlock(readbackBuffer) {
        const ctx = state.audioContext;
        const audioData = new Float32Array(readbackBuffer.getMappedRange());
        
        const audioBuffer = ctx.createBuffer(
            CONFIG.channels,
            DERIVED.samplesPerBlock,
            CONFIG.sampleRate
        );
        
        for (let ch = 0; ch < CONFIG.channels; ch++) {
            const channelData = audioBuffer.getChannelData(ch);
            const offset = ch * DERIVED.samplesPerBlock;
            for (let i = 0; i < DERIVED.samplesPerBlock; i++) {
                channelData[i] = audioData[offset + i];
            }
        }
        
        readbackBuffer.unmap();
        
        const source = ctx.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(state.gainNode);
        source.start(state.nextAudioTime);
        
        state.nextAudioTime += CONFIG.audioBlockDuration;
        state.audioFrame++;
    }

    // ============================================================================
    // Start
    // ============================================================================
    window.addEventListener('load', init);
    </script>
</body>
</html>